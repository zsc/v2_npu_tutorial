<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第4章：存储系统与数据流</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">NPU设计全流程教程：从算法到RTL实现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：NPU设计导论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：算法与算子分析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：量化与稀疏化技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：存储系统与数据流</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：脉动阵列原理与设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：脉动阵列RTL实现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：TPU编译器与映射</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：脉动阵列验证方法</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：数据流架构原理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：TSP微架构设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：数据流RTL实现</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：TSP编译器技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：多核扩展与互连</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：软硬件协同设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：性能分析与优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：工程实践与部署</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="4">第4章：存储系统与数据流</h1>
<p>在NPU设计中，存储系统往往是决定整体性能的关键瓶颈。本章深入探讨NPU存储层次设计、数据重用策略、DMA机制以及片上网络基础，帮助读者理解如何通过精心的存储系统设计来最大化NPU的计算效率。我们将以200 TOPS的设计目标为例，定量分析各级存储的带宽需求，并探讨不同数据流模式下的优化策略。</p>
<h2 id="41">4.1 存储层次设计</h2>
<h3 id="411">4.1.1 存储器技术对比</h3>
<p>NPU的存储层次通常包含三个主要层级，每层在容量、带宽、延迟和功耗方面都有不同的权衡：</p>
<p><strong>片上SRAM特性</strong></p>
<p>片上SRAM提供最高的带宽和最低的访问延迟，但容量受限且成本高昂。典型参数：</p>
<ul>
<li>访问延迟：1-2 cycles</li>
<li>带宽密度：~10 TB/s/mm²（7nm工艺）</li>
<li>功耗：~1 pJ/bit access</li>
<li>容量：通常10-100 MB（占芯片面积的30-50%）</li>
</ul>
<p>SRAM的物理实现通常采用6T或8T单元结构。6T SRAM单元包含6个晶体管，面积约为0.05 μm²（7nm工艺），而8T SRAM提供独立的读写端口，避免读干扰但面积增加约30%。在NPU设计中，通常采用混合策略：权重缓存使用高密度6T SRAM，而需要频繁读写的激活缓存使用8T SRAM。</p>
<p>对于200 TOPS的NPU，假设MAC利用率为80%，nvfp4精度下：
$$\text{所需带宽} = 200 \times 10^{12} \times 0.8 \times 3 \times 4 \text{ bits/s} = 1.92 \text{ Pb/s} = 240 \text{ TB/s}$$
其中因子3来自两个输入操作数和一个输出，因子4是nvfp4的位宽。</p>
<p>这个带宽需求远超单片SRAM所能提供，因此需要采用分布式SRAM架构。假设采用1024个SRAM bank，每个bank需要提供：
$$\text{Bank带宽} = \frac{240 \text{ TB/s}}{1024} = 234 \text{ GB/s}$$
在1 GHz时钟频率下，每个bank需要234位宽的接口，这在物理实现上是可行的。</p>
<p><strong>HBM技术演进</strong></p>
<p>HBM (High Bandwidth Memory) 提供了容量和带宽的良好平衡：</p>
<div class="codehilite"><pre><span></span><code>HBM代际对比：
         HBM2E    HBM3     HBM3E
带宽      460      819      1230    GB/s/stack
容量      16       24       36      GB/stack  
功耗效率   7        5.5      4.5     pJ/bit
I/O宽度   1024     1024     1024    bits
频率      3.6      6.4      9.6     Gbps/pin
电压      1.2      1.1      1.1     V
</code></pre></div>

<p>HBM采用2.5D封装技术，通过硅中介层（interposer）实现与主芯片的连接。每个HBM stack包含8-12层DRAM die，通过TSV（Through Silicon Via）垂直互连。对于200 TOPS的NPU设计，典型配置为4个HBM3 stack，提供总计3.2 TB/s的带宽和96 GB的容量。</p>
<p>HBM的关键优势在于其极短的物理距离（&lt;10mm）和宽I/O接口，相比GDDR6降低了约3倍的pJ/bit访问能耗。然而，HBM的成本约为DDR的5-8倍，且需要先进封装技术支持。</p>
<p><strong>DDR与GDDR权衡</strong></p>
<p>DDR提供大容量但带宽有限，GDDR则在两者间取得平衡：</p>
<ul>
<li>DDR5：~50 GB/s/channel，容量可达128GB/DIMM</li>
<li>GDDR6X：~80 GB/s/chip，容量2-4GB/chip</li>
<li>功耗：DDR ~20 pJ/bit，GDDR ~15 pJ/bit</li>
</ul>
<p>在自动驾驶场景中，需要存储大量的高精地图、历史轨迹和中间特征图。一个典型的BEV感知模型可能需要：</p>
<ul>
<li>输入缓存：6个相机 × 1920×1080×3 × 4 bytes = 150 MB</li>
<li>特征图缓存：多尺度特征 ~500 MB</li>
<li>历史帧缓存：10帧 × 200 MB = 2 GB</li>
<li>模型权重：~1 GB（INT8量化后）</li>
</ul>
<p>因此，边缘端NPU通常采用混合存储架构：</p>
<ol>
<li>片上SRAM（50 MB）：存储当前计算tile和部分权重</li>
<li>HBM/GDDR（8-16 GB）：存储完整模型和中间结果</li>
<li>DDR（32-64 GB）：存储历史数据和预取缓冲</li>
</ol>
<h3 id="412">4.1.2 存储带宽需求计算</h3>
<p><strong>算子带宽需求分析</strong></p>
<p>以矩阵乘法 $C = A \times B$ 为例，计算不同数据流模式的带宽需求：</p>
<p>设矩阵维度为 $M \times K$ 和 $K \times N$，计算量为 $2MNK$ FLOPs。</p>
<p>无重用情况下的理论带宽需求：
$$B_{\text{no-reuse}} = \frac{(MK + KN + MN) \times \text{bits}}{2MNK / \text{FLOPS}} = \frac{(M+N+K) \times \text{bits}}{2K} \times \text{FLOPS}$$
引入重用后，实际带宽需求取决于片上缓存大小 $S$：</p>
<ul>
<li>
<p>Output Stationary (OS)：缓存输出矩阵块
$$B_{OS} = \frac{2 \times \text{bits} \times \text{FLOPS}}{K}$$</p>
</li>
<li>
<p>Weight Stationary (WS)：缓存权重矩阵块
$$B_{WS} = \frac{2 \times \text{bits} \times \text{FLOPS}}{M}$$
<strong>实际案例：Transformer中的Attention计算</strong></p>
</li>
</ul>
<p>考虑自动驾驶中的多相机BEV Transformer，序列长度 $L = 10000$（100×100的BEV网格），嵌入维度 $d = 256$：</p>
<ol>
<li>
<p>Q、K、V投影：$3 \times L \times d^2$ 计算量
   - 带宽需求：$\frac{3Ld(L+d) \times 4 \text{ bits}}{3Ld^2 \times 2} = \frac{2(L+d)}{d} = 78.5$ bits/FLOP</p>
</li>
<li>
<p>Attention分数计算：$Q \times K^T$，计算量 $2L^2d$
   - 带宽需求：$\frac{2Ld \times 4 + L^2 \times 4}{2L^2d} = \frac{4}{d} + \frac{2}{L} ≈ 0.016$ bits/FLOP</p>
</li>
<li>
<p>加权求和：$\text{Softmax}(\text{scores}) \times V$，计算量 $2L^2d$
   - 带宽需求：类似于步骤2</p>
</li>
</ol>
<p>可见，Attention的不同阶段对带宽要求差异巨大，需要动态调整数据流策略。</p>
<p><strong>带宽效率指标</strong></p>
<p>定义带宽效率 $\eta_B$ 为实际计算吞吐量与理论峰值的比值：
$$\eta_B = \frac{\text{实际FLOPS}}{\min(\text{计算峰值}, \text{带宽} \times \text{算术强度})}$$
其中算术强度（Arithmetic Intensity）定义为：
$$AI = \frac{\text{FLOPs}}{\text{Bytes accessed}}$$
<strong>Roofline模型应用</strong></p>
<p>对于200 TOPS NPU，假设HBM带宽3.2 TB/s，构建Roofline模型：</p>
<ol>
<li>计算屋顶线：200 TOPS（nvfp4）</li>
<li>内存屋顶线：$3.2 \text{ TB/s} \times AI$</li>
<li>平衡点：$AI_{balance} = \frac{200 \times 10^{12}}{3.2 \times 10^{12}} = 62.5$ FLOPS/byte</li>
</ol>
<p>常见算子的算术强度：</p>
<ul>
<li>全连接层（1024×1024）：$AI = \frac{2 \times 1024^3}{3 \times 1024^2 \times 4} ≈ 171$ FLOPS/byte</li>
<li>1×1卷积：$AI ≈ \frac{2 \times C_{out}}{4}$ （假设输入已缓存）</li>
<li>3×3卷积：$AI ≈ \frac{2 \times 9 \times C_{out}}{4 \times 9 + 4}$ </li>
<li>Depthwise 3×3：$AI ≈ \frac{18}{40} = 0.45$ FLOPS/byte</li>
</ul>
<p>可见，depthwise卷积严重受限于内存带宽，而大矩阵乘法则受限于计算能力。</p>
<h3 id="413-bank">4.1.3 Bank冲突与访问模式优化</h3>
<p><strong>Bank组织结构</strong></p>
<p>典型的多Bank SRAM组织：</p>
<div class="codehilite"><pre><span></span><code>      ┌─────────────────────────┐
      │    Crossbar Switch      │
      └─────┬───┬───┬───┬───────┘
            │   │   │   │
         ┌──▼─┐ │   │ ┌──▼─┐
         │Bank│ │   │ │Bank│
         │ 0  │ │   │ │ N-1│
         └────┘ │   │ └────┘
              ┌─▼─┐ └─▼─┐
              │Bank│ ... │
              │ 1  │     │
              └────┘     │
</code></pre></div>

<p>Bank数量选择需要平衡面积开销和访问灵活性。常见配置：</p>
<ul>
<li>Bank数量：16-64</li>
<li>每Bank容量：64-256 KB</li>
<li>端口数：1R1W或2R1W</li>
</ul>
<p><strong>访问模式分析</strong></p>
<p>以2D卷积为例，不同的数据布局导致不同的访问模式：</p>
<ol>
<li>
<p>NCHW布局：连续访问同一通道的空间维度
   - 优点：空间局部性好
   - 缺点：通道间切换开销大</p>
</li>
<li>
<p>NHWC布局：连续访问同一位置的所有通道
   - 优点：适合depthwise操作
   - 缺点：大通道数时缓存利用率低</p>
</li>
<li>
<p>NCHWc布局（通道分块）：将C维度分块为C/c × c
   - 结合两种布局优点
   - c通常选择为SIMD宽度（如16或32）</p>
</li>
</ol>
<p><strong>实际访问模式优化案例</strong></p>
<p>以YOLOv8的C2f模块为例（输入256×80×80，输出256×80×80）：</p>
<p>采用NCHW布局时的Bank访问模式：</p>
<div class="codehilite"><pre><span></span><code><span class="err">时刻</span><span class="n">t</span><span class="o">:</span><span class="w">   </span><span class="n">Bank</span><span class="o">[</span><span class="mi">0</span><span class="o">]:</span><span class="w"> </span><span class="n">C0</span><span class="o">,</span><span class="n">H0</span><span class="o">,</span><span class="n">W0</span><span class="o">-</span><span class="n">W15</span>
<span class="w">         </span><span class="n">Bank</span><span class="o">[</span><span class="mi">1</span><span class="o">]:</span><span class="w"> </span><span class="n">C0</span><span class="o">,</span><span class="n">H0</span><span class="o">,</span><span class="n">W16</span><span class="o">-</span><span class="n">W31</span>
<span class="w">         </span><span class="o">...</span>
<span class="w">         </span><span class="n">Bank</span><span class="o">[</span><span class="mi">4</span><span class="o">]:</span><span class="w"> </span><span class="n">C0</span><span class="o">,</span><span class="n">H1</span><span class="o">,</span><span class="n">W0</span><span class="o">-</span><span class="n">W15</span>
</code></pre></div>

<p>问题：当卷积kernel跨行时，会访问Bank[0]和Bank[4]，可能造成冲突。</p>
<p>优化方案：</p>
<ol>
<li>Padding对齐：将宽度从80 padding到84（Bank数的倍数）</li>
<li>交织存储：$\text{Bank}_{id} = (C \times 7 + H \times 13 + W) \mod N_{banks}$</li>
<li>双缓冲：一个buffer用于当前行，另一个预取下一行</li>
</ol>
<p>这样可将Bank冲突率从15%降低到2%以下。</p>
<p><strong>冲突消除策略</strong></p>
<ol>
<li>
<p>地址交织（Interleaving）：
$$\text{Bank}_{\text{id}} = (\text{addr} &gt;&gt; \text{offset}) \bmod N_{\text{banks}}$$</p>
</li>
<li>
<p>素数Bank数量：选择素数个Bank（如17、31）减少规律性冲突</p>
</li>
<li>
<p>双缓冲（Double Buffering）：计算与数据传输重叠</p>
</li>
</ol>
<p><strong>Bank仲裁器设计</strong></p>
<p>多端口Bank访问需要仲裁机制，常见策略：</p>
<ol>
<li>固定优先级：计算单元 &gt; DMA &gt; 调试接口</li>
<li>轮询（Round-Robin）：公平但可能降低关键路径性能</li>
<li>加权轮询：根据请求源的带宽需求分配权重</li>
<li>信用机制（Credit-based）：每个请求源有信用额度</li>
</ol>
<p>仲裁延迟模型：
$$T_{access} = T_{arbitration} + T_{bank_access} + T_{routing}$$
典型值（1GHz时钟）：</p>
<ul>
<li>$T_{arbitration}$：1 cycle（简单仲裁）或2-3 cycles（复杂仲裁）</li>
<li>$T_{bank_access}$：1-2 cycles</li>
<li>$T_{routing}$：1 cycle（近距离）或2-3 cycles（跨片）</li>
</ul>
<p><strong>存储一致性保证</strong></p>
<p>在多个计算单元共享Bank时，需要保证数据一致性：</p>
<ol>
<li>写后读（RAW）hazard：通过scoreboard跟踪pending写操作</li>
<li>写后写（WAW）hazard：保证写操作的顺序</li>
<li>原子操作支持：实现atomic_add等操作用于累加</li>
</ol>
<p>硬件实现通常采用：</p>
<ul>
<li>版本号机制：每个地址关联版本号</li>
<li>锁机制：细粒度锁保护关键区域</li>
<li>事务内存：支持回滚的投机执行</li>
</ul>
<h2 id="42">4.2 数据重用模式</h2>
<p>数据重用是提高NPU能效的核心策略。通过最大化片上数据的重用次数，可以显著降低对外部存储器的访问需求，从而减少功耗并提高性能。</p>
<h3 id="421-temporal-vs-spatial">4.2.1 重用分类：Temporal vs Spatial</h3>
<p><strong>时间重用（Temporal Reuse）</strong></p>
<p>时间重用指同一数据在不同时间被同一计算单元多次使用：
$$\text{时间重用度} = \frac{\text{数据使用次数}}{\text{数据加载次数}}$$
以矩阵乘法为例，当计算 $C_{ij} = \sum_{k} A_{ik} \times B_{kj}$ 时：</p>
<ul>
<li>$A_{ik}$ 在计算 $C_{ij}$ 的所有 $j$ 值时被重用</li>
<li>$B_{kj}$ 在计算 $C_{ij}$ 的所有 $i$ 值时被重用</li>
</ul>
<p><strong>空间重用（Spatial Reuse）</strong></p>
<p>空间重用指同一数据同时被多个计算单元使用：</p>
<div class="codehilite"><pre><span></span><code>     广播数据
        │
    ┌───▼───┐
    │       │
  ┌─▼─┐   ┌─▼─┐
  │PE0│   │PE1│  ... │PEn│
  └───┘   └───┘      └───┘
</code></pre></div>

<p>空间重用的效率取决于：</p>
<ul>
<li>PE阵列的组织方式</li>
<li>数据广播网络的设计</li>
<li>计算的并行维度选择</li>
</ul>
<p><strong>重用优化的数学模型</strong></p>
<p>给定片上存储容量 $S$，优化目标是最小化数据传输量 $D$：
$$\min D = \sum_{i} \frac{s_i \times u_i}{r_i}$$
其中：</p>
<ul>
<li>$s_i$：数据 $i$ 的大小</li>
<li>$u_i$：数据 $i$ 的使用次数</li>
<li>$r_i$：数据 $i$ 的重用次数</li>
</ul>
<p>约束条件：$\sum_{i \in \text{on-chip}} s_i \leq S$</p>
<p><strong>具体示例：MobileNet中的Depthwise Separable卷积</strong></p>
<p>考虑MobileNetV3中的一个典型块，输入112×112×24，输出112×112×96：</p>
<ol>
<li>
<p>Depthwise 3×3：
   - 计算量：$112^2 \times 24 \times 9 \times 2 = 5.4M$ FLOPs
   - 数据量：</p>
<ul>
<li>输入：$112^2 \times 24 \times 4 = 1.2$ MB</li>
<li>权重：$3^2 \times 24 \times 4 = 0.9$ KB</li>
<li>输出：$112^2 \times 24 \times 4 = 1.2$ MB</li>
<li>算术强度：$\frac{5.4M}{2.4M} = 2.25$ FLOPs/byte</li>
</ul>
</li>
<li>
<p>Pointwise 1×1：
   - 计算量：$112^2 \times 24 \times 96 \times 2 = 57.8M$ FLOPs
   - 数据量：</p>
<ul>
<li>输入：$112^2 \times 24 \times 4 = 1.2$ MB（可重用上一步输出）</li>
<li>权重：$24 \times 96 \times 4 = 9.2$ KB</li>
<li>输出：$112^2 \times 96 \times 4 = 4.8$ MB</li>
<li>算术强度：$\frac{57.8M}{6.0M} = 9.6$ FLOPs/byte</li>
</ul>
</li>
</ol>
<p>优化策略：</p>
<ol>
<li>融合执行：将depthwise和pointwise融合，避免中间结果写回</li>
<li>空间tiling：将112×112分成7×7个16×16的tile</li>
<li>通道分块：将96个输出通道分成3批，每批32通道</li>
</ol>
<p>通过这些优化，可将总数据传输量从8.4 MB降低到2.1 MB，提高4倍性能。</p>
<h3 id="422-loop-tilingblocking">4.2.2 Loop Tiling与Blocking策略</h3>
<p><strong>基本Tiling原理</strong></p>
<p>Loop tiling将大的迭代空间分解为小块，使每块的工作集适配片上缓存：</p>
<p>原始循环：</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">M</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">            </span><span class="n">C</span><span class="o">[</span><span class="n">i</span><span class="o">][</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="o">[</span><span class="n">i</span><span class="o">][</span><span class="n">k</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="o">[</span><span class="n">k</span><span class="o">][</span><span class="n">j</span><span class="o">]</span>
</code></pre></div>

<p>Tiled版本：</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="n">i_outer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">M</span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="nl">T_i</span><span class="p">:</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">j_outer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="nl">T_j</span><span class="p">:</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">k_outer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="nl">T_k</span><span class="p">:</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="n">内层循环处理tile</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i_outer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">i_outer</span><span class="o">+</span><span class="n">T_i</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">j_outer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">j_outer</span><span class="o">+</span><span class="n">T_j</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k_outer</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">k_outer</span><span class="o">+</span><span class="n">T_k</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span>
<span class="w">                        </span><span class="n">C</span><span class="o">[</span><span class="n">i</span><span class="o">][</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="o">[</span><span class="n">i</span><span class="o">][</span><span class="n">k</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="o">[</span><span class="n">k</span><span class="o">][</span><span class="n">j</span><span class="o">]</span>
</code></pre></div>

<p><strong>Tile大小选择</strong></p>
<p>最优tile大小需要满足：</p>
<ol>
<li>容量约束：$T_i \times T_k + T_k \times T_j + T_i \times T_j \leq S$</li>
<li>带宽平衡：使计算时间与数据传输时间相匹配</li>
</ol>
<p>对于计算密度为 $\rho$ FLOPS/cycle 的NPU：
$$T_{\text{opt}} = \sqrt{\frac{S \times \rho \times f_{\text{clk}}}{3 \times BW}}$$
其中 $f_{\text{clk}}$ 是时钟频率，$BW$ 是内存带宽。</p>
<p><strong>多级Tiling</strong></p>
<p>针对多级存储层次，采用递归tiling：</p>
<div class="codehilite"><pre><span></span><code>L3 Cache Tile: 256×256×256
    │
    └─► L2 Cache Tile: 64×64×64
            │
            └─► L1 Cache Tile: 16×16×16
                    │
                    └─► Register Tile: 4×4×4
</code></pre></div>

<p>每级的tile大小比例通常遵循：
$$\frac{T_{L_{i+1}}}{T_{L_i}} \approx \sqrt[3]{\frac{S_{L_{i+1}}}{S_{L_i}}}$$
<strong>Tiling搜索空间优化</strong></p>
<p>对于一个6层嵌套循环的卷积操作（N, C, H, W, K, R, S），tiling参数空间为：
$$|\mathcal{S}| = \prod_{i=1}^{7} d_i$$
其中$d_i$是维度$i$的可能分块大小数。对于典型的224×224×256卷积层，搜索空间可达$10^{15}$。</p>
<p>常用优化方法：</p>
<ol>
<li>
<p>解析模型：基于屋顶线模型预测性能
$$T_{exec} = \max(T_{compute}, T_{memory})$$</p>
</li>
<li>
<p>机器学习方法：使用XGBoost/LSTM预测最优tiling
   - 训练数据：随机采样的10000个配置
   - 特征：tile大小、重用率、带宽需求
   - 目标：实测执行时间</p>
</li>
<li>
<p>遗传算法：平衡探索与利用
   - 初始种群：基于启发式规则
   - 适应度函数：$f = \alpha \times \text{FLOPS} - \beta \times \text{Energy}$
   - 变异策略：随机扰动tile大小</p>
</li>
</ol>
<p>实践中，通常结合三种方法：解析模型快速筛选，ML模型精细调优，遗传算法处理边缘情况。</p>
<h3 id="423-dataflowwsosrs">4.2.3 Dataflow分类：WS/OS/RS</h3>
<p><strong>Weight Stationary (WS)</strong></p>
<p>权重驻留数据流中，权重保持在PE本地，输入激活和部分和在PE间流动：</p>
<div class="codehilite"><pre><span></span><code><span class="err">特征：</span>

<span class="o">-</span><span class="w"> </span><span class="err">每个</span><span class="n">PE存储部分权重</span>
<span class="o">-</span><span class="w"> </span><span class="err">输入激活广播或单播</span>
<span class="o">-</span><span class="w"> </span><span class="err">部分和累加并传递</span>

<span class="err">优势：</span>

<span class="o">-</span><span class="w"> </span><span class="err">权重重用最大化</span>
<span class="o">-</span><span class="w"> </span><span class="err">适合</span><span class="n">batch</span><span class="w"> </span><span class="n">size大的场景</span>
<span class="o">-</span><span class="w"> </span><span class="err">减少权重读取能耗</span>

<span class="err">劣势：</span>

<span class="o">-</span><span class="w"> </span><span class="err">输入</span><span class="o">/</span><span class="err">输出数据传输开销大</span>
<span class="o">-</span><span class="w"> </span><span class="err">对</span><span class="n">irregular网络支持差</span>
</code></pre></div>

<p>数学表达：
$$\text{数据传输量}_{WS} = \text{Batch} \times (H \times W \times C_{in} + H \times W \times C_{out})$$
<strong>Output Stationary (OS)</strong></p>
<p>输出驻留数据流中，部分和保持在PE本地直到完成累加：</p>
<div class="codehilite"><pre><span></span><code>特征：

- 每个PE负责特定输出元素
- 权重和激活流经PE
- 部分和本地累加

优势：

- 部分和不需要传输
- 减少累加器带宽需求
- 适合深度网络

劣势：

- 权重和激活重用受限
- PE利用率可能不均
</code></pre></div>

<p>数学表达：
$$\text{数据传输量}_{OS} = K \times H \times W \times (C_{in} + C_{out})$$
<strong>Row Stationary (RS)</strong></p>
<p>行驻留数据流试图在PE内最大化所有数据类型的重用：</p>
<div class="codehilite"><pre><span></span><code>特征：

- 1D卷积原语映射到PE行
- 权重、激活、部分和都有重用
- 灵活支持不同层类型

优势：

- 能量效率最优
- 支持多种数据流模式
- 硬件利用率高

劣势：

- 控制复杂度高
- 编译器映射挑战大
</code></pre></div>

<p>能量模型比较：
$$E_{total} = E_{RF} \times N_{RF} + E_{NoC} \times N_{NoC} + E_{DRAM} \times N_{DRAM}$$
其中典型能量成本（45nm）：</p>
<ul>
<li>$E_{RF}$ ≈ 1 pJ</li>
<li>$E_{NoC}$ ≈ 2-6 pJ  </li>
<li>$E_{DRAM}$ ≈ 200 pJ</li>
</ul>
<p><strong>实际案例：Eyeriss v2的RS数据流实现</strong></p>
<p>Eyeriss v2采用分层式RS数据流，针对不同层类型自适应调整：</p>
<ol>
<li>
<p>CONV层映射：
   - 每个PE处理1D卷积（1×1×K）
   - 水平PE共享输入激活
   - 垂直PE共享部分和
   - 对角PE共享权重</p>
</li>
<li>
<p>FC层映射：
   - 切换到OS模式
   - 每个PE负责1个输出神经元
   - 权重广播至所有PE</p>
</li>
<li>
<p>Depthwise层映射：
   - 每个PE处理一个通道
   - 无需PE间通信
   - 最大化并行度</p>
</li>
</ol>
<p>能效对比（AlexNet CONV2层）：</p>
<ul>
<li>WS：1.4 TOPS/W</li>
<li>OS：1.6 TOPS/W  </li>
<li>RS：2.5 TOPS/W</li>
</ul>
<p>RS通过灵活的数据流切换，在不同层类型上都能达到较高效率。但代价是控制逻辑复杂度增加约40%，面积开销增加15%。</p>
<h2 id="43-dma">4.3 DMA设计与数据预取</h2>
<p>DMA（Direct Memory Access）是NPU中实现计算与数据传输重叠的关键组件。通过精心设计的DMA系统和预取策略，可以有效隐藏内存访问延迟，提高整体系统性能。</p>
<h3 id="431-dma">4.3.1 DMA架构设计</h3>
<p><strong>基本DMA架构</strong></p>
<p>现代NPU的DMA控制器通常包含以下组件：</p>
<div class="codehilite"><pre><span></span><code>┌─────────────────────────────────────┐
│         DMA Controller              │
├──────────┬──────────┬──────────────┤
│ Command  │ Address  │   Data       │
│  Queue   │Generator │   Buffers    │
├──────────┴──────────┴──────────────┤
│         Arbitration Logic           │
└──────────┬──────────────────────────┘
           │
     ┌─────▼─────┐
     │   Memory  │
     │ Interface │
     └───────────┘
</code></pre></div>

<p>关键设计参数：</p>
<ul>
<li>通道数量：典型8-32个独立DMA通道</li>
<li>突发长度：64-256字节（匹配内存系统）</li>
<li>队列深度：16-64个待处理请求</li>
<li>支持模式：线性、2D/3D块传输、scatter-gather</li>
</ul>
<p><strong>多通道DMA设计考量</strong></p>
<p>在自动驾驶NPU中，不同通道通常专门用于特定数据类型：</p>
<ol>
<li>
<p>相机数据通道（6个）：
   - 实时从MIPI/GMSL接口接收
   - 时延要求：&lt; 10ms
   - 带宽：6 × 4MP × 30fps × 12bit = 3.5 GB/s</p>
</li>
<li>
<p>特征图通道（4个）：
   - 在多个网络层间传输中间特征
   - 支持2D/3D tensor转置
   - 带宽：~20 GB/s</p>
</li>
<li>
<p>权重加载通道（2个）：
   - 预加载下一层权重
   - 支持2:4稀疏解压
   - 带宽：~5 GB/s</p>
</li>
<li>
<p>输出通道（2个）：
   - 将结果写回主存
   - 支持NMS后处理
   - 带宽：~2 GB/s</p>
</li>
</ol>
<p>通道优先级设置：
$$P_i = \alpha_i \times \text{latency_sensitivity} + \beta_i \times \text{bandwidth_demand}$$
其中相机数据通道的$\alpha$最高，确保实时性。</p>
<p><strong>带宽分配策略</strong></p>
<p>多通道DMA的带宽分配可建模为优化问题：
$$\max \sum_{i=1}^{N} w_i \times \min(r_i, b_i)$$
约束条件：$\sum_{i=1}^{N} b_i \leq B_{total}$</p>
<p>其中：</p>
<ul>
<li>$w_i$：通道 $i$ 的优先级权重</li>
<li>$r_i$：通道 $i$ 的请求带宽</li>
<li>$b_i$：分配给通道 $i$ 的带宽</li>
<li>$B_{total}$：总可用带宽</li>
</ul>
<p><strong>地址生成模式</strong></p>
<p>支持复杂访问模式的地址生成器：</p>
<ol>
<li>
<p>线性模式：
$$\text{addr} = \text{base} + \text{offset} \times \text{stride}$$</p>
</li>
<li>
<p>2D块模式：
$$\text{addr} = \text{base} + x \times \text{stride}_x + y \times \text{stride}_y$$</p>
</li>
<li>
<p>循环缓冲模式：
$$\text{addr} = \text{base} + (\text{offset} \bmod \text{size})$$</p>
</li>
<li>
<p>3D tensor模式（用于BEV特征）：
$$\text{addr} = \text{base} + z \times S_z + y \times S_y + x \times S_x$$
其中$S_z = H \times W \times C$, $S_y = W \times C$, $S_x = C$</p>
</li>
<li>
<p>稀疏索引模式（用于2:4稀疏）：
$$\text{addr} = \text{base} + \text{index}[i] \times \text{elem_size}$$
<strong>地址生成器的硬件实现</strong></p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="w">     </span><span class="err">┌─────────────────┐</span>
<span class="w">     </span><span class="err">│</span><span class="w">  </span><span class="n">Base</span><span class="w"> </span><span class="n">Register</span><span class="w">  </span><span class="err">│</span>
<span class="w">     </span><span class="err">└───────┬─────────┘</span>
<span class="w">              </span><span class="err">│</span>
<span class="w">     </span><span class="err">┌────────┴─────────┐</span>
<span class="w">     </span><span class="err">│</span><span class="w">   </span><span class="n">Counter</span><span class="w"> </span><span class="n">Bank</span><span class="w">   </span><span class="err">│</span>
<span class="w">     </span><span class="err">│</span><span class="w">  </span><span class="o">[</span><span class="n">X</span><span class="o">][</span><span class="n">Y</span><span class="o">][</span><span class="n">Z</span><span class="o">][</span><span class="n">N</span><span class="o">]</span><span class="w">    </span><span class="err">│</span>
<span class="w">     </span><span class="err">└───────┬──────────┘</span>
<span class="w">              </span><span class="err">│</span>
<span class="w">     </span><span class="err">┌────────┴─────────┐</span>
<span class="w">     </span><span class="err">│</span><span class="w">  </span><span class="n">Stride</span><span class="w"> </span><span class="n">Multiply</span><span class="w"> </span><span class="err">│</span>
<span class="w">     </span><span class="err">└────────┬─────────┘</span>
<span class="w">              </span><span class="err">│</span>
<span class="w">     </span><span class="err">┌────────┴─────────┐</span>
<span class="w">     </span><span class="err">│</span><span class="w">   </span><span class="n">Accumulator</span><span class="w">    </span><span class="err">│</span>
<span class="w">     </span><span class="err">└────────┬─────────┘</span>
<span class="w">              </span><span class="err">│</span>
<span class="w">         </span><span class="n">Final</span><span class="w"> </span><span class="n">Address</span>
</code></pre></div>

<p>关键优化：</p>
<ul>
<li>使用移位代替乘法（当stride为2的幂）</li>
<li>多级计数器级联，减少进位链延迟</li>
<li>预计算常用stride组合</li>
</ul>
<h3 id="432">4.3.2 描述符与链表管理</h3>
<p><strong>描述符格式</strong></p>
<p>典型的DMA描述符包含：</p>
<div class="codehilite"><pre><span></span><code><span class="nx">struct</span><span class="w"> </span><span class="nx">DMADescriptor</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">uint64_t</span><span class="w"> </span><span class="nx">src_addr</span><span class="p">;</span><span class="w">      </span><span class="c1">// 源地址</span>
<span class="w">    </span><span class="nx">uint64_t</span><span class="w"> </span><span class="nx">dst_addr</span><span class="p">;</span><span class="w">      </span><span class="c1">// 目标地址</span>
<span class="w">    </span><span class="nx">uint32_t</span><span class="w"> </span><span class="nx">length</span><span class="p">;</span><span class="w">        </span><span class="c1">// 传输长度</span>
<span class="w">    </span><span class="nx">uint16_t</span><span class="w"> </span><span class="nx">src_stride</span><span class="p">;</span><span class="w">    </span><span class="c1">// 源跨步</span>
<span class="w">    </span><span class="nx">uint16_t</span><span class="w"> </span><span class="nx">dst_stride</span><span class="p">;</span><span class="w">    </span><span class="c1">// 目标跨步</span>
<span class="w">    </span><span class="nx">uint8_t</span><span class="w">  </span><span class="nx">burst_len</span><span class="p">;</span><span class="w">     </span><span class="c1">// 突发长度</span>
<span class="w">    </span><span class="nx">uint8_t</span><span class="w">  </span><span class="nx">flags</span><span class="p">;</span><span class="w">         </span><span class="c1">// 控制标志</span>
<span class="w">    </span><span class="nx">uint64_t</span><span class="w"> </span><span class="nx">next_desc</span><span class="p">;</span><span class="w">     </span><span class="c1">// 下一描述符指针</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>链表管理机制</strong></p>
<p>支持多种链表组织方式：</p>
<ol>
<li>单链表：简单但灵活性有限</li>
<li>循环链表：适合重复的传输模式</li>
<li>树形结构：支持条件分支和复杂控制流</li>
</ol>
<p>描述符预取优化：</p>
<ul>
<li>Shadow寄存器：预加载下一个描述符</li>
<li>描述符缓存：减少重复读取开销</li>
<li>批量更新：减少描述符写回次数</li>
</ul>
<p><strong>描述符压缩技术</strong></p>
<p>为减少描述符存储开销，可采用压缩编码：</p>
<ol>
<li>
<p>差分编码：存储与前一描述符的差值
$$\text{desc}_i = \text{desc}_{i-1} + \Delta_i$$</p>
</li>
<li>
<p>模板编码：预定义常用模板
   - 模板0：连续线性传输
   - 模板1：2D块传输（图像）
   - 模板2：3D tensor传输</p>
</li>
<li>
<p>位域压缩：减少未使用位</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>原始：64bit地址 + 32bit长度 + 16bit stride = 112 bits
压缩：32bit基址 + 16bit偏移 + 16bit长度 + 8bit stride = 72 bits
</code></pre></div>

<p>通过这些技术，可将描述符存储开销减少40-60%。</p>
<h3 id="433">4.3.3 预取策略与隐藏延迟</h3>
<p><strong>静态预取策略</strong></p>
<p>编译时确定的预取模式：</p>
<div class="codehilite"><pre><span></span><code>计算时间线：|--Compute Layer N--|--Compute Layer N+1--|
数据传输：      |--Prefetch N+1--|    |--Prefetch N+2--|
</code></pre></div>

<p>预取距离计算：
$$D_{prefetch} = \lceil \frac{L_{mem}}{T_{compute}} \rceil$$
其中：</p>
<ul>
<li>$L_{mem}$：内存访问延迟</li>
<li>$T_{compute}$：计算一个tile的时间</li>
</ul>
<p><strong>动态预取策略</strong></p>
<p>运行时自适应的预取机制：</p>
<ol>
<li>
<p>基于历史的预测：
$$\text{addr}_{next} = \text{addr}_{current} + \alpha \times \Delta_{history}$$</p>
</li>
<li>
<p>基于stride的检测：
   - 检测连续访问的stride模式
   - 自动触发相应的预取请求</p>
</li>
</ol>
<p><strong>双缓冲与流水线</strong></p>
<p>双缓冲机制的时序分析：</p>
<div class="codehilite"><pre><span></span><code>Buffer A: |--Load--|--Compute--|--Store--|
Buffer B:          |--Load--|--Compute--|--Store--|
时间:     0       T        2T        3T        4T
</code></pre></div>

<p>理想情况下的效率：
$$\eta_{pipeline} = \frac{T_{compute}}{\max(T_{compute}, T_{load}, T_{store})}$$
实现200 TOPS的系统，假设：</p>
<ul>
<li>计算密度：1000 GFLOPS/s</li>
<li>所需数据量：100 GB/s</li>
<li>DMA延迟：100 cycles</li>
</ul>
<p>则需要的预取深度：
$$N_{prefetch} = \lceil \frac{100 \text{ cycles} \times 1 \text{ GHz}}{1000 \text{ GFLOPS} / 100 \text{ GB/s}} \rceil = 10$$
<strong>预取性能建模</strong></p>
<p>考虑预取准确率和覆盖率：
$$\text{Speedup} = \frac{1}{(1-h) + \frac{h}{1+\alpha \times p}}$$
其中：</p>
<ul>
<li>$h$：预取命中率</li>
<li>$p$：预取覆盖率（隐藏的延迟比例）  </li>
<li>$\alpha$：预取开销系数</li>
</ul>
<p>实际测量数据（ResNet-50）：</p>
<ul>
<li>静态预取：$h=0.95$, $p=0.8$, 加速1.7倍</li>
<li>动态预取：$h=0.85$, $p=0.9$, 加速1.6倍</li>
<li>混合策略：$h=0.92$, $p=0.85$, 加速1.75倍</li>
</ul>
<h2 id="44-noc">4.4 片上网络(NoC)基础</h2>
<p>片上网络是NPU内部各计算单元、存储单元和I/O接口之间的通信基础设施。良好的NoC设计对于实现高效的数据流和低延迟通信至关重要。在200 TOPS级别的NPU设计中，NoC需要提供数百TB/s的聚合带宽，同时保持纳秒级的延迟。</p>
<h3 id="441-noc">4.4.1 NoC拓扑结构</h3>
<p><strong>常见拓扑对比</strong></p>
<div class="codehilite"><pre><span></span><code>总线(Bus):          环形(Ring):        网格(Mesh):
┌─┬─┬─┬─┬─┐        ┌──┐──┌──┐        ┌──┬──┬──┐
│ │ │ │ │ │        │  └──┘  │        │  │  │  │
└─┴─┴─┴─┴─┘        │        │        ├──┼──┼──┤
                    │  ┌──┐  │        │  │  │  │
                    └──┘  └──┘        ├──┼──┼──┤
                                      │  │  │  │
                                      └──┴──┴──┘

Torus(环面):        Fat Tree(胖树):     Crossbar(交叉开关):
┌──┬──┬──┐         ╱─┴─╲               ┌─┬─┬─┬─┐
│╲ │╱ │╲ │         ╱     ╲              ├─┼─┼─┼─┤
├──┼──┼──┤        ┌─┐   ┌─┐            ├─┼─┼─┼─┤
│╱ │╲ │╱ │        │ │   │ │            ├─┼─┼─┼─┤
├──┼──┼──┤        └┬┘   └┬┘            └─┴─┴─┴─┘
│╲ │╱ │╲ │         │     │
└──┴──┴──┘        PE    PE
</code></pre></div>

<p>性能特征比较：
| 拓扑 | 直径 | 分割带宽 | 成本 | 扩展性 | 功耗复杂度 |</p>
<table>
<thead>
<tr>
<th>拓扑</th>
<th>直径</th>
<th>分割带宽</th>
<th>成本</th>
<th>扩展性</th>
<th>功耗复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td>总线</td>
<td>1</td>
<td>O(1)</td>
<td>低</td>
<td>差</td>
<td>O(N)</td>
</tr>
<tr>
<td>环形</td>
<td>N/2</td>
<td>O(2)</td>
<td>低</td>
<td>中</td>
<td>O(N)</td>
</tr>
<tr>
<td>2D网格</td>
<td>2√N</td>
<td>O(√N)</td>
<td>中</td>
<td>好</td>
<td>O(N)</td>
</tr>
<tr>
<td>Torus</td>
<td>√N</td>
<td>O(2√N)</td>
<td>高</td>
<td>好</td>
<td>O(N)</td>
</tr>
<tr>
<td>胖树</td>
<td>logN</td>
<td>O(N)</td>
<td>高</td>
<td>优秀</td>
<td>O(NlogN)</td>
</tr>
<tr>
<td>Crossbar</td>
<td>1</td>
<td>O(N)</td>
<td>极高</td>
<td>差</td>
<td>O(N²)</td>
</tr>
</tbody>
</table>
<p><strong>拓扑选择的定量分析</strong></p>
<p>对于200 TOPS的NPU设计，需要支持：</p>
<ul>
<li>计算单元：256-1024个MAC阵列</li>
<li>存储带宽：240 TB/s（内部）</li>
<li>节点间通信：~50 TB/s</li>
</ul>
<p>拓扑选择的约束条件：
$$\text{Cost} = \alpha \times N_{routers} + \beta \times N_{links} + \gamma \times \text{Wire_length}$$
其中：</p>
<ul>
<li>$N_{routers} = N$（节点数）</li>
<li>$N_{links} = k \times N / 2$（k为节点度数）</li>
<li>Wire_length取决于拓扑和物理布局</li>
</ul>
<p>以16×16 2D Mesh为例的详细分析：</p>
<ul>
<li>节点数：256</li>
<li>链路数：480（内部） + 64（边界）</li>
<li>最大跳数：30</li>
<li>平均跳数：10.67</li>
<li>分割带宽：16 × 单链路带宽</li>
</ul>
<p>若每条链路32位宽，运行在2GHz，则：
$$BW_{link} = 32 \text{ bits} \times 2 \text{ GHz} = 8 \text{ GB/s}$$
$$BW_{bisection} = 16 \times 8 = 128 \text{ GB/s}$$
<strong>层次化NoC设计</strong></p>
<p>现代NPU通常采用层次化NoC，结合多种拓扑：</p>
<div class="codehilite"><pre><span></span><code>全局NoC (2D Mesh)
    │
├───┼───────────┐
│   │           │
▼   ▼           ▼
局部集群       局部集群
(Crossbar)     (Ring)
│   │          │   │
PE  PE        PE  PE
</code></pre></div>

<p>层次化设计的优势：</p>
<ol>
<li>局部通信低延迟（1-2 cycles）</li>
<li>全局通信高带宽</li>
<li>功耗优化（局部通信功耗低）</li>
<li>面积效率（减少长距离布线）</li>
</ol>
<p><strong>拓扑的物理实现考虑</strong></p>
<p>在7nm工艺下的布线资源：</p>
<ul>
<li>Metal层数：15-17层</li>
<li>低层金属（M1-M4）：局部互连，间距45-56nm</li>
<li>中层金属（M5-M8）：中等距离，间距80-100nm  </li>
<li>高层金属（M9-M15）：全局布线，间距200-360nm</li>
</ul>
<p>2D Mesh的物理布局优化：</p>
<ol>
<li>折叠布局：减少最长线延迟</li>
<li>对角线增强：添加对角链路减少跳数</li>
<li>Express通道：跨多跳的快速通道</li>
</ol>
<h3 id="442">4.4.2 路由算法与流控</h3>
<p><strong>维序路由(Dimension-Order Routing)</strong></p>
<p>最常用的死锁避免路由算法，先沿X维路由，再沿Y维：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">xy_routing</span><span class="p">(</span><span class="n">src_x</span><span class="p">,</span> <span class="n">src_y</span><span class="p">,</span> <span class="n">dst_x</span><span class="p">,</span> <span class="n">dst_y</span><span class="p">):</span>
    <span class="c1"># 先X后Y，避免死锁</span>
    <span class="n">path</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># X维路由</span>
    <span class="k">while</span> <span class="n">src_x</span> <span class="o">!=</span> <span class="n">dst_x</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">src_x</span> <span class="o">&lt;</span> <span class="n">dst_x</span><span class="p">:</span>
            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;EAST&quot;</span><span class="p">)</span>
            <span class="n">src_x</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;WEST&quot;</span><span class="p">)</span> 
            <span class="n">src_x</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="c1"># Y维路由</span>
    <span class="k">while</span> <span class="n">src_y</span> <span class="o">!=</span> <span class="n">dst_y</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">src_y</span> <span class="o">&lt;</span> <span class="n">dst_y</span><span class="p">:</span>
            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;NORTH&quot;</span><span class="p">)</span>
            <span class="n">src_y</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;SOUTH&quot;</span><span class="p">)</span>
            <span class="n">src_y</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">path</span>
</code></pre></div>

<p>XY路由的死锁避免证明：</p>
<ul>
<li>通道依赖图(CDG)无环</li>
<li>东西向通道优先级高于南北向</li>
<li>不会形成循环等待</li>
</ul>
<p><strong>自适应路由</strong></p>
<p>根据网络拥塞动态选择路径：
$$P_{route} = \arg\min_{p \in \text{paths}} \sum_{l \in p} C_l$$
其中$C_l$是链路$l$的拥塞度量，可定义为：
$$C_l = \alpha \times Q_l + \beta \times U_l + \gamma \times D_l$$</p>
<ul>
<li>$Q_l$：链路$l$的队列占用率</li>
<li>$U_l$：链路利用率（过去N周期平均）</li>
<li>$D_l$：链路传输延迟</li>
<li>$\alpha, \beta, \gamma$：权重系数</li>
</ul>
<p><strong>部分自适应路由算法</strong></p>
<ol>
<li>West-First路由：先向西，然后自适应选择</li>
<li>North-Last路由：最后向北，之前自适应</li>
<li>Odd-Even路由：奇偶列采用不同限制</li>
</ol>
<p>Odd-Even路由规则：</p>
<ul>
<li>偶数列：禁止E→N和E→S转向</li>
<li>奇数列：禁止N→W和S→W转向</li>
</ul>
<p>这保证了无死锁同时提供路径多样性。</p>
<p><strong>容错路由</strong></p>
<p>处理故障节点和链路的路由策略：</p>
<div class="codehilite"><pre><span></span><code>故障节点绕行示例：
┌──┬──┬──┐
│  │  │  │
├──┼XX┼──┤  XX: 故障节点
│  │↗↘│  │  绕行路径：→↗→
├──┴──┴──┤            →↘→
</code></pre></div>

<p>容错路由的实现：</p>
<ol>
<li>故障表维护：每个路由器维护邻居状态</li>
<li>动态重路由：检测到故障后更新路由表</li>
<li>多路径冗余：预先计算备用路径</li>
</ol>
<p><strong>虚通道(Virtual Channel)</strong></p>
<p>通过多个虚拟通道共享物理链路，提高利用率并避免死锁：</p>
<div class="codehilite"><pre><span></span><code>物理链路
┌─────────────────────┐
│  VC0: [████    ]    │  请求消息
│  VC1: [  ████  ]    │  响应消息
│  VC2: [      ████]  │  多播消息
│  VC3: [██      ]    │  逃逸通道
└─────────────────────┘
</code></pre></div>

<p>虚通道分配策略：</p>
<ul>
<li>静态分配：不同消息类型使用固定VC</li>
<li>动态分配：基于信用的VC分配</li>
<li>逃逸通道：保留一个VC用于死锁恢复</li>
</ul>
<p><strong>虚通道的硬件实现</strong></p>
<div class="codehilite"><pre><span></span><code>输入端口结构：
         ┌──────────────┐
输入 ──&gt; │ VC Demux     │
         ├──────────────┤
         │ VC0 Buffer   │
         ├──────────────┤
         │ VC1 Buffer   │
         ├──────────────┤
         │ VC2 Buffer   │
         ├──────────────┤
         │ VC3 Buffer   │
         ├──────────────┤
         │ VC Allocator │
         ├──────────────┤
         │ Switch Alloc │
         └──────────────┘
                │
              交叉开关
</code></pre></div>

<p>VC分配的两阶段过程：</p>
<ol>
<li>VC分配阶段：为包头flit分配输出VC</li>
<li>开关分配阶段：为数据flit分配交叉开关时隙</li>
</ol>
<p>分配器的仲裁算法：</p>
<ul>
<li>iSLIP：迭代轮询匹配</li>
<li>PIM：并行迭代匹配</li>
<li>波前仲裁：对角线扫描</li>
</ul>
<h3 id="443">4.4.3 流控机制</h3>
<p><strong>信用流控(Credit-based Flow Control)</strong></p>
<p>接收方向发送方发放信用，控制数据发送：</p>
<div class="codehilite"><pre><span></span><code>发送方                     接收方
┌──────┐                 ┌──────┐
│Buffer│ ──data(3 flits)─&gt; │Buffer│
│Count │ &lt;──credit(3)──── │Free  │
│=N-3  │                 │=3    │
└──────┘                 └──────┘
</code></pre></div>

<p>信用计算的详细推导：
$$\text{Credits}_{min} = \lceil \frac{\text{RTT} \times BW}{\text{flit_size}} \rceil + \text{buffer_depth}$$
其中RTT (Round-Trip Time)包括：</p>
<ul>
<li>前向延迟：$T_{fwd} = T_{router} + T_{link} + T_{deserialize}$</li>
<li>信用返回延迟：$T_{credit} = T_{process} + T_{link_back}$</li>
<li>总RTT = $T_{fwd} + T_{credit}$</li>
</ul>
<p>对于2GHz时钟，128位flit的系统：</p>
<ul>
<li>RTT = 8 cycles（典型值）</li>
<li>链路带宽 = 256 Gbps</li>
<li>最小信用数 = $\lceil \frac{8 \times 256}{128} \rceil = 16$</li>
</ul>
<p><strong>On/Off流控</strong></p>
<p>更简单但效率较低的流控机制：</p>
<div class="codehilite"><pre><span></span><code>时序图：
发送方:<span class="w"> </span><span class="o">|--</span><span class="k">Send</span><span class="o">--|--</span><span class="k">Wait</span><span class="o">--|--</span><span class="k">Send</span><span class="o">--|</span>
接收方:<span class="w"> </span><span class="o">|--</span><span class="nv">Recv</span><span class="o">--|--</span><span class="nv">OFF</span><span class="o">----|--</span><span class="nv">ON</span><span class="o">----|</span>
</code></pre></div>

<p>On/Off流控的阈值设置：</p>
<ul>
<li>OFF阈值：$T_{off} = B_{total} - \text{RTT} \times \text{Rate}$</li>
<li>ON阈值：$T_{on} = T_{off} / 2$（避免频繁切换）</li>
</ul>
<p><strong>背压机制(Backpressure)</strong></p>
<p>当下游拥塞时，向上游传播停止信号：
$$\text{Throughput} = \min_{i \in \text{path}}(\text{capacity}_i \times (1 - \text{congestion}_i))$$
背压传播模型：
$$P_{stop}(t+1, n) = \begin{cases}
1 &amp; \text{if } Q_n(t) &gt; T_{high} \\
P_{stop}(t, n+1) &amp; \text{if } Q_n(t) &gt; T_{mid} \\
0 &amp; \text{otherwise}
\end{cases}$$
其中$Q_n(t)$是节点$n$在时刻$t$的队列占用。</p>
<p><strong>弹性缓冲流控(Elastic Buffer)</strong></p>
<p>利用流水线寄存器作为分布式缓冲：</p>
<div class="codehilite"><pre><span></span><code>┌──┐  ┌──┐  ┌──┐  ┌──┐
│R1│──│R2│──│R3│──│R4│  弹性流水线
└──┘  └──┘  └──┘  └──┘
 ↓     ↓     ↓     ↓
Valid &amp; Ready握手
</code></pre></div>

<p>优势：</p>
<ul>
<li>降低缓冲需求</li>
<li>减少面积开销</li>
<li>提高时钟频率</li>
</ul>
<h3 id="444-noc">4.4.4 NoC性能建模</h3>
<p><strong>延迟模型</strong></p>
<p>端到端延迟的精确建模：
$$L_{total} = L_{header} + \sum_{i=1}^{H}(L_{router,i} + L_{link,i}) + L_{contention}$$
各组件延迟分解：</p>
<ol>
<li>
<p>路由器延迟（4级流水线）：
   - 路由计算(RC)：1 cycle
   - VC分配(VA)：1 cycle<br />
   - 开关分配(SA)：1 cycle
   - 交叉开关传输(ST)：1 cycle</p>
</li>
<li>
<p>链路延迟：
$$L_{link} = \lceil \frac{d}{v_{signal}} \times f_{clk} \rceil$$
其中$v_{signal} \approx 0.5c$（硅中信号速度）</p>
</li>
<li>
<p>竞争延迟（排队理论）：
$$L_{contention} = \frac{1}{\mu - \lambda}$$
其中$\mu$是服务率，$\lambda$是到达率。</p>
</li>
</ol>
<p><strong>带宽模型</strong></p>
<p>有效带宽的详细分析：
$$BW_{effective} = BW_{physical} \times \eta_{total}$$
其中：
$$\eta_{total} = \eta_{protocol} \times \eta_{routing} \times \eta_{congestion} \times \eta_{serialization}$$
各效率因子：</p>
<ul>
<li>$\eta_{protocol} = \frac{\text{payload_size}}{\text{packet_size}}$（典型0.8-0.9）</li>
<li>$\eta_{routing} = 1 - P_{misroute}$（典型0.85-0.95）</li>
<li>$\eta_{congestion} = (1 - \rho)^2$（$\rho$是网络负载）</li>
<li>$\eta_{serialization} = \frac{W_{flit}}{W_{phit}}$（flit到phit的转换效率）</li>
</ul>
<p><strong>热点和拥塞建模</strong></p>
<p>热点形成的概率模型：
$$P_{hotspot} = 1 - (1 - p)^N$$
其中$p$是单个节点成为热点的概率，$N$是节点数。</p>
<p>拥塞扩散模型（基于流体动力学）：
$$\frac{\partial \rho}{\partial t} + \nabla \cdot (\rho \mathbf{v}) = S$$
其中：</p>
<ul>
<li>$\rho$：流量密度</li>
<li>$\mathbf{v}$：流速向量</li>
<li>$S$：源项（注入/移除流量）</li>
</ul>
<p><strong>实际案例：Google TPU v4的ICI</strong></p>
<p>TPU v4使用3D Torus拓扑的Inter-Chip Interconnect (ICI)：</p>
<p>架构参数：</p>
<ul>
<li>4096个芯片（64×64 2D Torus）</li>
<li>每芯片6个100 Gbps光链路</li>
<li>总分割带宽：4.8 Tbps</li>
<li>平均跳数：32（理论）、38（实际，含拥塞）</li>
<li>单跳延迟：50ns</li>
<li>端到端延迟：&lt; 2μs（轻载）、&lt; 5μs（重载）</li>
</ul>
<p>关键优化技术：</p>
<ol>
<li>
<p><strong>自适应路由避免热点</strong>
   - 基于全局拥塞表的路由决策
   - 每100μs更新一次拥塞信息
   - 减少热点概率80%</p>
</li>
<li>
<p><strong>多轨道(Multi-rail)设计</strong>
   - 6条独立物理通道
   - 不同流量类型分配到不同轨道
   - 有效带宽提升4.5倍</p>
</li>
<li>
<p><strong>硬件集合通信原语</strong>
   - AllReduce：340 GB/s（4096节点）
   - AllGather：450 GB/s
   - Reduce-Scatter：380 GB/s
   - 相比软件实现加速10-20倍</p>
</li>
<li>
<p><strong>拥塞感知的流调度</strong>
   - ECN (Explicit Congestion Notification)标记
   - 自适应注入率控制
   - 优先级反转避免饥饿</p>
</li>
</ol>
<p>性能测试结果（BERT-Large训练）：</p>
<ul>
<li>通信效率：92%（通信时间/计算时间）</li>
<li>扩展效率：85%（4096芯片相对单芯片）</li>
<li>能效：15 TFLOPS/W（含通信）</li>
</ul>
<h2 id="_1">本章小结</h2>
<p>本章深入探讨了NPU存储系统与数据流设计的核心概念。我们从存储层次设计出发，分析了片上SRAM、HBM和DDR的技术特征与权衡，通过定量计算确定了200 TOPS NPU所需的240 TB/s内部带宽。在数据重用模式部分，我们对比了时间重用与空间重用，详细分析了WS/OS/RS三种数据流的能效特征。DMA设计章节介绍了多通道DMA架构、描述符管理和预取策略，展示了如何通过双缓冲实现计算与传输的重叠。最后，我们深入研究了片上网络的拓扑结构、路由算法和流控机制，并以TPU v4的ICI为例说明了大规模互连的实际实现。</p>
<p>关键要点：</p>
<ol>
<li>存储带宽是NPU性能的主要瓶颈，需要通过多级存储层次和数据重用来缓解</li>
<li>数据流模式的选择直接影响能效，RS数据流通过灵活切换达到最优</li>
<li>DMA预取深度需要根据内存延迟和计算吞吐量精确计算</li>
<li>NoC设计需要在延迟、带宽和成本间权衡，2D Mesh是主流选择</li>
<li>虚通道和信用流控是避免死锁和提高利用率的关键机制</li>
</ol>
<p>关键公式回顾：</p>
<ul>
<li>Roofline平衡点：$AI_{balance} = \frac{\text{Peak_FLOPS}}{\text{Memory_BW}}$</li>
<li>时间重用度：$R_t = \frac{\text{使用次数}}{\text{加载次数}}$</li>
<li>最优tile大小：$T_{opt} = \sqrt{\frac{S \times \rho \times f_{clk}}{3 \times BW}}$</li>
<li>NoC端到端延迟：$L_{total} = L_{header} + L_{router} \times H + L_{link} \times H + L_{contention}$</li>
<li>信用流控：$Credits_{min} = \lceil \frac{RTT \times BW}{flit_size} \rceil + buffer_depth$</li>
</ul>
<h2 id="_2">练习题</h2>
<h3 id="_3">基础题</h3>
<p><strong>练习4.1</strong> 计算存储带宽需求
一个NPU需要执行矩阵乘法 $C_{512×512} = A_{512×768} \times B_{768×512}$，采用nvfp4量化。若计算吞吐量为100 TOPS，MAC利用率80%，计算：
a) 无数据重用时的理论带宽需求
b) 采用Output Stationary数据流的带宽需求
c) 若片上SRAM仅32MB，设计合理的tiling策略</p>
<details>
<summary>提示</summary>
<p>考虑计算量与数据传输量的比值，注意nvfp4为4位数据类型。对于tiling，需要满足三个矩阵块都能装入SRAM。</p>
</details>
<details>
<summary>答案</summary>
<p>a) 无数据重用时：</p>
<ul>
<li>计算量：$2 \times 512 \times 768 \times 512 = 402.7M$ FLOPs</li>
<li>数据量：$(512×768 + 768×512 + 512×512) \times 4 \text{ bits} = 4.19M$ bytes</li>
<li>算术强度：$\frac{402.7M}{4.19M} = 96$ FLOPs/byte</li>
<li>带宽需求：$\frac{100 \text{ TOPS} \times 0.8}{96} = 833$ GB/s</li>
</ul>
<p>b) Output Stationary：</p>
<ul>
<li>每个输出元素计算时，A的一行和B的一列各读取一次</li>
<li>重用因子：K = 768</li>
<li>带宽需求：$\frac{833 \text{ GB/s}}{768/3} = 3.25$ GB/s</li>
</ul>
<p>c) Tiling策略：</p>
<ul>
<li>设tile大小为 $T_M × T_K × T_N$</li>
<li>约束：$(T_M × T_K + T_K × T_N + T_M × T_N) × 4 \text{ bits} \leq 32 \text{ MB}$</li>
<li>选择：$T_M = T_N = 128, T_K = 192$</li>
<li>验证：$(128×192 + 192×128 + 128×128) × 0.5 = 32.25$ KB &lt; 32 MB ✓</li>
</ul>
</details>
<p><strong>练习4.2</strong> Bank冲突分析
一个NPU有16个SRAM bank，采用交织存储，地址映射为 $Bank_{id} = addr \bmod 16$。现需要同时访问地址0x1000、0x1010、0x1020、0x1030，问：
a) 是否存在bank冲突？
b) 若改为 $Bank_{id} = (addr &gt;&gt; 4) \bmod 16$，结果如何？
c) 设计一个映射函数避免此类冲突</p>
<details>
<summary>提示</summary>
<p>计算每个地址对应的bank编号，检查是否有重复。注意地址的二进制表示。</p>
</details>
<details>
<summary>答案</summary>
<p>a) 原始映射：</p>
<ul>
<li>0x1000 mod 16 = 0</li>
<li>0x1010 mod 16 = 0  </li>
<li>0x1020 mod 16 = 0</li>
<li>0x1030 mod 16 = 0
存在严重冲突，4个地址都映射到Bank 0</li>
</ul>
<p>b) 改进映射：</p>
<ul>
<li>(0x1000 &gt;&gt; 4) mod 16 = 0x100 mod 16 = 0</li>
<li>(0x1010 &gt;&gt; 4) mod 16 = 0x101 mod 16 = 1</li>
<li>(0x1020 &gt;&gt; 4) mod 16 = 0x102 mod 16 = 2  </li>
<li>(0x1030 &gt;&gt; 4) mod 16 = 0x103 mod 16 = 3
无冲突，分别映射到Bank 0,1,2,3</li>
</ul>
<p>c) 更好的映射函数（XOR哈希）：
$Bank_{id} = ((addr &gt;&gt; 4) \oplus (addr &gt;&gt; 8)) \bmod 16$
这样可以打散规律性访问模式</p>
</details>
<p><strong>练习4.3</strong> DMA预取深度计算
一个NPU系统的参数如下：</p>
<ul>
<li>计算一个256×256矩阵乘法tile需要500 cycles</li>
<li>HBM访问延迟：200 cycles</li>
<li>DMA传输256×256×4bits数据需要100 cycles
计算最小预取深度以完全隐藏内存延迟。</li>
</ul>
<details>
<summary>提示</summary>
<p>考虑流水线执行，预取需要覆盖整个内存访问时间。</p>
</details>
<details>
<summary>答案</summary>
<p>总内存访问时间：200 + 100 = 300 cycles
计算时间：500 cycles</p>
<p>由于计算时间大于内存访问时间，理论上1级预取即可：</p>
<ul>
<li>时刻0-300：预取tile 1，计算tile 0</li>
<li>时刻300-500：继续计算tile 0</li>
<li>时刻500-800：预取tile 2，计算tile 1</li>
</ul>
<p>但考虑到可能的延迟变化，实际需要2级预取缓冲：
$$N_{prefetch} = \lceil \frac{300}{500} \rceil + 1 = 2$$</p>
<p>这样可以容忍最多500 cycles的延迟抖动。</p>
</details>
<h3 id="_4">挑战题</h3>
<p><strong>练习4.4</strong> 多级存储优化
设计一个三级存储系统用于Transformer的Attention计算（序列长度8192，头维度64）：</p>
<ul>
<li>L1: 256KB per PE，延迟1 cycle</li>
<li>L2: 8MB shared，延迟10 cycles</li>
<li>HBM: 16GB，延迟100 cycles
要求设计数据分块和调度策略，最小化总延迟。</li>
</ul>
<details>
<summary>提示</summary>
<p>Attention包含QK^T和Score×V两个大矩阵乘法，考虑Flash Attention的分块策略。</p>
</details>
<details>
<summary>答案</summary>
<p>采用Flash Attention分块策略：</p>
<ol>
<li>外层循环：将8192序列分成32个块，每块256</li>
<li>中层循环：Q和K的块大小为256×64</li>
<li>内层循环：累加部分attention scores</li>
</ol>
<p>存储分配：</p>
<ul>
<li>L1：当前Q块(256×64×4B = 64KB) + 部分K块(64KB) + 中间结果(128KB)</li>
<li>L2：预取下一个K/V块对 + Softmax归一化因子</li>
<li>HBM：完整Q、K、V矩阵</li>
</ul>
<p>调度策略：</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="n">q_block</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">外层</span>
<span class="w">    </span><span class="k">load</span><span class="w"> </span><span class="n">Q</span><span class="o">[</span><span class="n">q_block</span><span class="o">]</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">L2</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">kv_block</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">中层</span><span class="w">  </span>
<span class="w">        </span><span class="n">prefetch</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">kv_block+1</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="o">[</span><span class="n">kv_block+1</span><span class="o">]</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">L2</span>
<span class="w">        </span><span class="k">load</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">kv_block</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="o">[</span><span class="n">kv_block</span><span class="o">]</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">L1</span>
<span class="w">        </span><span class="k">compute</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Q</span><span class="o">[</span><span class="n">q_block</span><span class="o">]</span><span class="w"> </span><span class="err">@</span><span class="w"> </span><span class="n">K</span><span class="o">[</span><span class="n">kv_block</span><span class="o">]</span><span class="p">.</span><span class="n">T</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">在L1</span>
<span class="w">        </span><span class="k">compute</span><span class="w"> </span><span class="n">P</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">softmax</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">在L1</span>
<span class="w">        </span><span class="k">compute</span><span class="w"> </span><span class="n">O</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">P</span><span class="w"> </span><span class="err">@</span><span class="w"> </span><span class="n">V</span><span class="o">[</span><span class="n">kv_block</span><span class="o">]</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">累加到L1</span>
<span class="w">    </span><span class="k">write</span><span class="w"> </span><span class="n">O</span><span class="w"> </span><span class="n">back</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">HBM</span>
</code></pre></div>

<p>总延迟估算：</p>
<ul>
<li>L1访问：32×32×256×64×2 = 33M次，33M cycles</li>
<li>L2访问：32×32×2 = 2K次，20K cycles  </li>
<li>HBM访问：32×3 = 96次，9.6K cycles</li>
<li>计算：32×32×(256×256×64×2) = 8.6G FLOPs</li>
</ul>
<p>在100 GFLOPS的PE上，计算主导，总时间约86ms。</p>
</details>
<p><strong>练习4.5</strong> NoC路由优化
在8×8 2D Mesh上，从(0,0)发送数据到(7,7)，同时存在以下流量：</p>
<ul>
<li>(0,7)→(7,0): 大流量</li>
<li>(3,3)→(4,4): 中等流量
设计自适应路由策略避免拥塞。</li>
</ul>
<details>
<summary>提示</summary>
<p>分析XY路由的冲突点，考虑YX或部分自适应路由。</p>
</details>
<details>
<summary>答案</summary>
<p>XY路由分析：</p>
<ul>
<li>(0,0)→(7,7): 路径(0,0)→(7,0)→(7,7)</li>
<li>(0,7)→(7,0): 路径(0,7)→(7,7)→(7,0)  </li>
<li>冲突：两条路径在(7,0)-(7,7)段重叠</li>
</ul>
<p>自适应策略：</p>
<ol>
<li>监测(7,*)行的拥塞</li>
<li>
<p>当拥塞度&gt;阈值时，(0,0)→(7,7)改用YX路由：
   (0,0)→(0,7)→(7,7)</p>
</li>
<li>
<p>中间节点(3,3)-(4,4)的流量较小，维持XY路由</p>
</li>
</ol>
<p>负载均衡算法：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">adaptive_route</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">congestion_map</span><span class="p">):</span>
    <span class="n">xy_path</span> <span class="o">=</span> <span class="n">compute_xy_path</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
    <span class="n">yx_path</span> <span class="o">=</span> <span class="n">compute_yx_path</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>

    <span class="n">xy_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">congestion_map</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">xy_path</span><span class="p">)</span>
    <span class="n">yx_cost</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">congestion_map</span><span class="p">[</span><span class="n">link</span><span class="p">]</span> <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">yx_path</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">yx_cost</span> <span class="o">&lt;</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">xy_cost</span><span class="p">:</span>  <span class="c1"># 20%改善阈值</span>
        <span class="k">return</span> <span class="n">yx_path</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">xy_path</span>
</code></pre></div>

<p>预期改善：</p>
<ul>
<li>最大链路利用率从90%降至60%</li>
<li>平均延迟减少35%</li>
<li>吞吐量提升40%</li>
</ul>
</details>
<p><strong>练习4.6</strong> 数据流能效分析
比较三种数据流(WS/OS/RS)在以下场景的能效：</p>
<ul>
<li>1×1卷积，输入224×224×128，输出224×224×256</li>
<li>3×3 Depthwise卷积，224×224×256</li>
<li>全连接层，输入2048，输出1000
假设：RF访问1pJ，NoC传输5pJ，DRAM访问200pJ。</li>
</ul>
<details>
<summary>提示</summary>
<p>计算每种数据流的数据传输量和重用模式，估算总能耗。</p>
</details>
<details>
<summary>答案</summary>
<p><strong>1×1卷积分析：</strong></p>
<p>WS (Weight Stationary):</p>
<ul>
<li>权重驻留：128×256 = 32K次RF访问</li>
<li>输入流动：224×224×128 = 6.4M次NoC</li>
<li>输出累加：224×224×256 = 12.8M次NoC</li>
<li>能耗：32K×1 + 19.2M×5 = 96 mJ</li>
</ul>
<p>OS (Output Stationary):</p>
<ul>
<li>输出驻留：224×224×256 = 12.8M次RF访问</li>
<li>权重广播：128×256×(224×224/PE数) 次NoC</li>
<li>假设256个PE：128×256×196 = 6.4M次NoC</li>
<li>输入广播：类似6.4M次NoC</li>
<li>能耗：12.8M×1 + 12.8M×5 = 76.8 mJ</li>
</ul>
<p>RS (Row Stationary):</p>
<ul>
<li>1D卷积映射，每个PE处理部分输入输出</li>
<li>本地RF：~8M次访问</li>
<li>NoC传输：~8M次</li>
<li>能耗：8M×1 + 8M×5 = 48 mJ</li>
</ul>
<p><strong>3×3 Depthwise分析：</strong></p>
<p>WS：不适用（无权重共享）</p>
<p>OS：</p>
<ul>
<li>每个输出像素需要9个输入</li>
<li>RF：224×224×256 = 12.8M</li>
<li>NoC：224×224×256×9 = 115M</li>
<li>能耗：12.8M×1 + 115M×5 = 587 mJ</li>
</ul>
<p>RS：</p>
<ul>
<li>每个PE处理一个通道</li>
<li>RF：224×224×256 = 12.8M  </li>
<li>NoC：最小（仅边界像素）~2M</li>
<li>能耗：12.8M×1 + 2M×5 = 22.8 mJ</li>
</ul>
<p><strong>全连接层分析：</strong></p>
<p>WS：</p>
<ul>
<li>权重驻留：2048×1000 = 2M次RF</li>
<li>输入广播到所有PE</li>
<li>能耗：2M×1 + 输入广播开销</li>
</ul>
<p>OS：</p>
<ul>
<li>输出驻留：1000次RF</li>
<li>权重和输入流动</li>
<li>能耗：取决于batch size</li>
</ul>
<p>RS：</p>
<ul>
<li>介于WS和OS之间</li>
<li>能耗：~1.5× min(WS, OS)</li>
</ul>
<p>结论：</p>
<ul>
<li>1×1卷积：RS &gt; OS &gt; WS</li>
<li>Depthwise：RS &gt;&gt; OS (WS不适用)</li>
<li>全连接：取决于batch size，通常WS较优</li>
</ul>
</details>
<p><strong>练习4.7</strong> 存储一致性设计
设计一个支持多个计算集群共享SRAM的一致性协议，要求：</p>
<ul>
<li>支持原子读-改-写操作</li>
<li>最小化同步开销</li>
<li>避免死锁</li>
</ul>
<details>
<summary>提示</summary>
<p>考虑目录式或监听式协议，注意原子操作的实现。</p>
</details>
<details>
<summary>答案</summary>
<p>采用简化的MESI协议变体：</p>
<p>状态定义：</p>
<ul>
<li>M (Modified): 独占且已修改</li>
<li>E (Exclusive): 独占未修改</li>
<li>S (Shared): 共享只读</li>
<li>I (Invalid): 无效</li>
</ul>
<p>协议设计：</p>
<ol>
<li>
<p>读操作：
   - Invalid → Shared (广播读请求)
   - Shared/Exclusive/Modified → 直接读</p>
</li>
<li>
<p>写操作：
   - Invalid/Shared → Exclusive (广播失效)
   - Exclusive → Modified (本地写)
   - Modified → 直接写</p>
</li>
<li>
<p>原子操作(Atomic RMW)：</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="nx">acquire_lock</span><span class="p">(</span><span class="kd">addr</span><span class="p">):</span>
<span class="w">  </span><span class="k">while</span><span class="w"> </span><span class="nx">True</span><span class="p">:</span>
<span class="w">    </span><span class="nx">state</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">get_state</span><span class="p">(</span><span class="kd">addr</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="nx">Modified</span><span class="p">:</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="nx">try_upgrade_to_modified</span><span class="p">(</span><span class="kd">addr</span><span class="p">):</span>
<span class="w">        </span><span class="k">break</span>
<span class="w">    </span><span class="nx">backoff</span><span class="p">()</span>

<span class="nx">atomic_add</span><span class="p">(</span><span class="kd">addr</span><span class="p">,</span><span class="w"> </span><span class="nx">value</span><span class="p">):</span>
<span class="w">  </span><span class="nx">acquire_lock</span><span class="p">(</span><span class="kd">addr</span><span class="p">)</span>
<span class="w">  </span><span class="nx">old</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">read</span><span class="p">(</span><span class="kd">addr</span><span class="p">)</span>
<span class="w">  </span><span class="nx">write</span><span class="p">(</span><span class="kd">addr</span><span class="p">,</span><span class="w"> </span><span class="nx">old</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">value</span><span class="p">)</span>
<span class="w">  </span><span class="nx">release_lock</span><span class="p">(</span><span class="kd">addr</span><span class="p">)</span>
</code></pre></div>

<p>死锁避免：</p>
<ol>
<li>锁排序：按地址顺序获取</li>
<li>超时机制：等待超过阈值则回退</li>
<li>专用原子操作单元：避免与普通访问竞争</li>
</ol>
<p>硬件实现：</p>
<ul>
<li>目录表：2K entries，4-way组相联</li>
<li>状态位：2 bits per cache line</li>
<li>同步网络：专用低延迟网络</li>
<li>原子单元：每个cluster一个，支持16个pending操作</li>
</ul>
<p>性能指标：</p>
<ul>
<li>读延迟：1-3 cycles (取决于状态)</li>
<li>写延迟：3-10 cycles (需要失效)</li>
<li>原子操作：10-20 cycles</li>
<li>面积开销：&lt; 5%总SRAM面积</li>
</ul>
</details>
<p><strong>练习4.8</strong> 端到端系统设计
为自动驾驶BEV感知设计完整的存储和NoC系统：</p>
<ul>
<li>输入：6路相机，每路4MP@30fps</li>
<li>网络：BEVFormer-Base</li>
<li>实时性要求：&lt; 100ms延迟
给出详细的架构设计和带宽分配。</li>
</ul>
<details>
<summary>提示</summary>
<p>分析BEVFormer的计算和存储需求，设计pipeline确保实时性。</p>
</details>
<details>
<summary>答案</summary>
<p><strong>系统需求分析：</strong></p>
<p>输入带宽：</p>
<ul>
<li>6 × 4MP × 30fps × 12bit = 10.8 Gbps</li>
</ul>
<p>BEVFormer计算：</p>
<ul>
<li>Backbone (ResNet-50): 4 GFLOPS × 6 = 24 GFLOPS</li>
<li>Neck (FPN): 2 GFLOPS × 6 = 12 GFLOPS  </li>
<li>Transformer: 15 GFLOPS</li>
<li>Head: 3 GFLOPS</li>
<li>总计：54 GFLOPS，需要540 TOPS@100ms</li>
</ul>
<p>存储需求：</p>
<ul>
<li>输入缓冲：6 × 4MP × 2 = 48 MB (双缓冲)</li>
<li>特征图：~200 MB</li>
<li>BEV Query：100×100×256×4 = 10 MB</li>
<li>模型权重：150 MB (INT8)</li>
<li>历史特征：10帧 × 50 MB = 500 MB</li>
</ul>
<p><strong>架构设计：</strong></p>
<div class="codehilite"><pre><span></span><code>┌─────────────────────────────────┐
│     相机接口 (6× MIPI CSI-2)     │
└──────────┬──────────────────────┘
           │ 10.8 Gbps
    ┌──────▼──────────┐
    │  输入缓冲SRAM   │ 48 MB
    │   (双缓冲)      │
    └──────┬──────────┘
           │
    ┌──────▼──────────────────────┐
    │   2D Mesh NoC (16×16)       │
    │   链路: 256 Gbps             │
    │   总带宽: 4 Tbps             │
    └──┬───────────────────────┬──┘
       │                       │
┌──────▼──────┐       ┌────────▼────────┐
│ 计算集群×4  │       │  共享L2 SRAM    │
│ 150 TOPS    │       │    128 MB       │
│ L1: 8MB     │       └─────────────────┘
└─────────────┘                │
                               │
                        ┌──────▼──────┐
                        │  HBM3 16GB  │
                        │  819 GB/s   │
                        └─────────────┘
</code></pre></div>

<p><strong>流水线设计：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nx">Stage</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">图像预处理</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="nx">ms</span><span class="p">)</span>

<span class="o">-</span><span class="w"> </span><span class="mi">6</span><span class="nx">路并行处理</span>
<span class="o">-</span><span class="w"> </span><span class="nx">畸变校正</span><span class="err">、</span><span class="nx">归一化</span>

<span class="nx">Stage</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="nx">Backbone特征提取</span><span class="w"> </span><span class="p">(</span><span class="mi">30</span><span class="nx">ms</span><span class="p">)</span><span class="w">  </span>

<span class="o">-</span><span class="w"> </span><span class="mi">6</span><span class="nx">路CNN并行</span>
<span class="o">-</span><span class="w"> </span><span class="nx">特征金字塔生成</span>

<span class="nx">Stage</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span><span class="w"> </span><span class="nx">BEV转换</span><span class="w"> </span><span class="p">(</span><span class="mi">20</span><span class="nx">ms</span><span class="p">)</span>

<span class="o">-</span><span class="w"> </span><span class="mi">3</span><span class="nx">D</span><span class="err">→</span><span class="mi">2</span><span class="nx">D投影</span>
<span class="o">-</span><span class="w"> </span><span class="nx">多尺度特征融合</span>

<span class="nx">Stage</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="nx">Temporal融合</span><span class="w"> </span><span class="p">(</span><span class="mi">15</span><span class="nx">ms</span><span class="p">)</span>

<span class="o">-</span><span class="w"> </span><span class="nx">历史BEV特征对齐</span>
<span class="o">-</span><span class="w"> </span><span class="nx">运动补偿</span>

<span class="nx">Stage</span><span class="w"> </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="nx">Transformer</span><span class="w"> </span><span class="p">(</span><span class="mi">20</span><span class="nx">ms</span><span class="p">)</span>

<span class="o">-</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">attention</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Cross</span><span class="o">-</span><span class="nx">attention</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">images</span>

<span class="nx">Stage</span><span class="w"> </span><span class="mi">6</span><span class="p">:</span><span class="w"> </span><span class="nx">检测头</span><span class="w"> </span><span class="p">(</span><span class="mi">5</span><span class="nx">ms</span><span class="p">)</span>

<span class="o">-</span><span class="w"> </span><span class="mi">3</span><span class="nx">D框回归</span>
<span class="o">-</span><span class="w"> </span><span class="nx">类别预测</span>
</code></pre></div>

<p><strong>带宽分配：</strong></p>
<ol>
<li>相机→输入缓冲：10.8 Gbps (持续)</li>
<li>输入缓冲→计算集群：50 GB/s (突发)</li>
<li>计算集群↔L2：200 GB/s</li>
<li>L2↔HBM：100 GB/s (平均)</li>
<li>NoC内部：500 GB/s (峰值)</li>
</ol>
<p><strong>优化策略：</strong></p>
<ol>
<li>相机帧重叠：N+1帧预处理与N帧推理并行</li>
<li>特征缓存：重用backbone特征3帧</li>
<li>量化：INT8 backbone，FP16 transformer</li>
<li>稀疏化：attention mask剪枝50%</li>
<li>多分辨率：远处低分辨率，近处高分辨率</li>
</ol>
<p><strong>性能预期：</strong></p>
<ul>
<li>延迟：85ms (含预处理)</li>
<li>吞吐量：11.7 fps</li>
<li>功耗：45W</li>
<li>能效：12 TOPS/W</li>
</ul>
</details>
<h2 id="gotchas">常见陷阱与错误 (Gotchas)</h2>
<h3 id="_5">存储设计陷阱</h3>
<ol>
<li>
<p><strong>带宽计算错误</strong>
   - 错误：只考虑计算带宽，忽略控制和同步开销
   - 正确：预留20-30%带宽用于控制、同步和非理想因素</p>
</li>
<li>
<p><strong>Bank冲突低估</strong>
   - 错误：假设均匀分布的访问模式
   - 正确：分析实际访问模式，考虑stride访问造成的冲突</p>
</li>
<li>
<p><strong>忽视数据对齐</strong>
   - 错误：任意的数据布局和tile大小
   - 正确：确保数据对齐到缓存行边界，tile大小是SIMD宽度的倍数</p>
</li>
</ol>
<h3 id="_6">数据流设计陷阱</h3>
<ol start="4">
<li>
<p><strong>单一数据流策略</strong>
   - 错误：所有层使用相同的数据流
   - 正确：根据层类型自适应切换数据流模式</p>
</li>
<li>
<p><strong>重用机会错失</strong>
   - 错误：独立优化每个维度的重用
   - 正确：联合优化时间和空间重用，考虑数据生命周期</p>
</li>
</ol>
<h3 id="dma">DMA设计陷阱</h3>
<ol start="6">
<li>
<p><strong>预取距离不当</strong>
   - 错误：固定的预取距离
   - 正确：根据计算时间和内存延迟动态调整</p>
</li>
<li>
<p><strong>描述符链表死锁</strong>
   - 错误：循环依赖的描述符链
   - 正确：确保描述符DAG无环，使用超时机制</p>
</li>
</ol>
<h3 id="noc">NoC设计陷阱</h3>
<ol start="8">
<li>
<p><strong>路由死锁</strong>
   - 错误：自适应路由without escape channel
   - 正确：保证至少一个虚通道使用确定性无死锁路由</p>
</li>
<li>
<p><strong>热点忽视</strong>
   - 错误：假设均匀流量分布
   - 正确：识别和处理同步点、规约操作造成的热点</p>
</li>
<li>
<p><strong>信用泄露</strong></p>
<ul>
<li>错误：信用计数器溢出或不匹配</li>
<li>正确：使用饱和计数器，定期同步信用</li>
</ul>
</li>
</ol>
<h2 id="_7">最佳实践检查清单</h2>
<h3 id="_8">存储系统设计审查</h3>
<ul>
<li>[ ] <strong>容量规划</strong></li>
<li>[ ] 各级存储容量是否满足最大工作集需求？</li>
<li>[ ] 是否为双缓冲预留了空间？</li>
<li>
<p>[ ] 碎片化损失是否在可接受范围（&lt;20%）？</p>
</li>
<li>
<p>[ ] <strong>带宽匹配</strong></p>
</li>
<li>[ ] 计算带宽与存储带宽是否平衡？</li>
<li>[ ] 是否识别了带宽瓶颈？</li>
<li>
<p>[ ] 峰值带宽需求是否有缓冲机制？</p>
</li>
<li>
<p>[ ] <strong>访问模式优化</strong></p>
</li>
<li>[ ] 是否分析了主要kernel的访问模式？</li>
<li>[ ] Bank冲突率是否&lt;5%？</li>
<li>[ ] 是否实现了合适的交织策略？</li>
</ul>
<h3 id="_9">数据流优化审查</h3>
<ul>
<li>[ ] <strong>重用最大化</strong></li>
<li>[ ] 是否量化了各级数据重用率？</li>
<li>[ ] Tiling参数是否经过优化？</li>
<li>
<p>[ ] 是否考虑了融合执行机会？</p>
</li>
<li>
<p>[ ] <strong>调度优化</strong></p>
</li>
<li>[ ] 计算与数据传输是否充分重叠？</li>
<li>[ ] 是否消除了不必要的数据移动？</li>
<li>[ ] 关键路径是否已识别和优化？</li>
</ul>
<h3 id="dma_1">DMA配置审查</h3>
<ul>
<li>[ ] <strong>通道分配</strong></li>
<li>[ ] DMA通道数是否充足？</li>
<li>[ ] 优先级设置是否合理？</li>
<li>
<p>[ ] 是否支持所需的传输模式？</p>
</li>
<li>
<p>[ ] <strong>延迟隐藏</strong></p>
</li>
<li>[ ] 预取深度是否充分？</li>
<li>[ ] 双缓冲是否正确实现？</li>
<li>[ ] 是否处理了内存延迟变化？</li>
</ul>
<h3 id="noc_1">NoC验证审查</h3>
<ul>
<li>[ ] <strong>功能正确性</strong></li>
<li>[ ] 路由算法是否无死锁？</li>
<li>[ ] 是否处理了所有边界情况？</li>
<li>
<p>[ ] 容错机制是否完备？</p>
</li>
<li>
<p>[ ] <strong>性能验证</strong></p>
</li>
<li>[ ] 是否达到了目标带宽？</li>
<li>[ ] 延迟是否满足要求？</li>
<li>
<p>[ ] 负载均衡是否有效？</p>
</li>
<li>
<p>[ ] <strong>扩展性</strong></p>
</li>
<li>[ ] 是否支持目标规模？</li>
<li>[ ] 扩展时性能下降是否可接受？</li>
<li>[ ] 是否预留了升级接口？</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter3.html" class="nav-link prev">← 第3章：量化与稀疏化技术</a><a href="chapter5.html" class="nav-link next">第5章：脉动阵列原理与设计 →</a></nav>
        </main>
    </div>
</body>
</html>